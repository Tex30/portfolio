---
title: "Data Wrangling Assessment Task 3: Dataset challenge"
author: "Michael Teixeira S4133975"
subtitle: 
output: 
  word_document: default
  html_document:
    df_print: paged
  pdf: default
---

## 1. Setup 

#### 1.1 Load libraries:
```{r, message=FALSE}
library(tidyverse)
library(knitr) 
library(stringr)
library(magrittr)  
library(chron)
library(lubridate)
library(skimr)

```

## 2. Introduction 

For this analysis, we are using three datasets from VicRoads Open Data[01]:
  
* ACCIDENT.csv: Contains detailed information about individual road accidents in Victoria.
  - ACCIDENT_NO: Unique identifier for the accident.
  - ACCIDENTDATE: Date when the accident occurred.
  - ACCIDENTTIME: Time of the accident.
  - ACCIDENT_TYPE: Numeric code for the type of accident (1-9).
  - ACCIDENT_TYPE_DESCRIPTION: Description of the accident type.
  - DAY_OF_WEEK: Numeric code for the day of the week.
  - DAY_OF_WEEK_DESCRIPTION: Description of the day of the week.
  - DCA_CODE: Numeric code for accident classification.
  - DCA_CODE_DESCRIPTION: Description of the accident classification.
  - LIGHT_CONDITION: Numeric code for light condition at the time of the accident (1-9).
  - LIGHT_CONDITION_DESCRIPTION: Description of the light condition.
  - NODE_ID: Unique identifier for the accident location.
  - NO_OF_VEHICLES: Number of vehicles involved in the accident.
  - NO_PERSONS: Total number of people involved in the accident.
  - NO_PERSONS_INJ_2: Number of people with serious injuries.
  - NO_PERSONS_INJ_3: Number of people with other injuries.
  - NO_PERSONS_KILLED: Number of people killed.
  - NO_PERSONS_NOT_INJ: Number of people with no injuries.
  - POLICE_ATTEND: Indicates police attendance (1=Yes, 2=No, 9=Unknown).
  - ROAD_GEOMETRY: Numeric code for road layout where the accident occurred.
  - ROAD_GEOMETRY_DESCRIPTION: Description of the road layout.
  - SEVERITY: Severity of the accident in numeric code.
  - SPEED_ZONE: Speed zone at the accident location.    
    
* ATMOSPHERIC_COND.csv: Includes data about weather conditions during accidents. 
  - ACCIDENT_NO: Unique identifier for the accident.
  - ATMOSPH_COND: Code for weather and atmospheric conditions.
  - ATMOSPH_COND_SEQ: Sequence number for multiple atmospheric conditions in the same incident.
  - ATMOSPH_COND_Desc: Description of atmospheric conditions (e.g., Clear, Raining, Snowing).   
      
* ROAD_SURFACE_COND.csv: Contains information about the condition of the road surface during accidents.   
  - ACCIDENT_NO: Unique identifier for the accident.
  - SURFACE_COND: Numeric code for road surface conditions.
  - SURFACE_COND_Desc: Description of road surface conditions (e.g., Dry, Wet, Muddy).
  - SURFACE_COND_SEQ: Sequence number for multiple road surface conditions in the same incident.    

These datasets will allow us to explore the relationships between road accidents, weather conditions, and road surface conditions in Victoria.

```{r}

# Importing datasets:
accidents <- read.csv("ACCIDENT.csv")
atmospheric <- read.csv("ATMOSPHERIC_COND.csv")
road_surface <- read.csv("ROAD_SURFACE_COND.csv")

# Glimpse of each dataset:
glimpse(accidents)
glimpse(atmospheric)
glimpse(road_surface)

```

## 3. Understand our Datasets 

We begin by inspecting the data types and structure of the datasets to understand their content and identify any issues. This step helps ensure that data is in the correct format for analysis.


```{r, results='hide'}
# Inspect data types and structure
str(accidents)    # 169877 obs. of  23 variables
str(atmospheric)  # 172120 obs. of  4 variables
str(road_surface) # 170839 obs. of  4 variables
```


```{r, results='hide'}
# Summary statistics
summary(accidents)
summary(atmospheric)
summary(road_surface)
```




##	4. Tyding the Data 

We clean and restructure the datasets by removing unnecessary columns, renaming variables for consistency, and merging the datasets.

#### 4.1 Drop unnecessary variables and rename remaining variables

```{r,message=FALSE}
# Function to convert to lowercase:
lower_case <- function(x) {
  x %>% tolower()}

# Drop unnecessary variables and rename remaining variables in accidents dataset:
accidents %<>%
  select(-DAY_OF_WEEK, -ACCIDENT_TYPE, -RMA, -ROAD_GEOMETRY, -DCA_CODE, -NODE_ID) %>%
  rename(
    accident_id = ACCIDENT_NO,
    date = ACCIDENT_DATE,
    time = ACCIDENT_TIME,
    type_desc = ACCIDENT_TYPE_DESC,
    day_of_week = DAY_WEEK_DESC,
    desc_CA = DCA_DESC,
    total_people_involved = NO_PERSONS,
    serious_injuries = NO_PERSONS_INJ_2,
    minor_injuries = NO_PERSONS_INJ_3,
    fatalities = NO_PERSONS_KILLED,
    uninjured_people = NO_PERSONS_NOT_INJ
  )%>%
rename_all(lower_case)


# Drop unnecessary variables and rename remaining variables in atmospheric dataset:
atmospheric %<>%
  select(-ATMOSPH_COND, -ATMOSPH_COND_SEQ) %>%
  rename(
    accident_id = ACCIDENT_NO,
    weather_condition = ATMOSPH_COND_DESC
  )

# Drop unnecessary variables and rename remaining variables in road_surface dataset:
road_surface %<>%
  select(-SURFACE_COND, -SURFACE_COND_SEQ) %>%
  rename(
    accident_id = ACCIDENT_NO,
    surface_condition = SURFACE_COND_DESC
  )
```
#### 4.2 Merge datasets

```{r,message=FALSE}
# Merge datasets
merged_data <- accidents %>%
  left_join(atmospheric, by = "accident_id") %>%
  left_join(road_surface, by = "accident_id")
```

#### 4.3 Convert variables to appropriate types
```{r,message=FALSE}
# Convert merged dataset variables to appropriate types
merged_data %<>%
  mutate(
    accident_id = as.factor(accident_id),
    date = as.Date(date, format = "%Y-%m-%d"),
    time = times(time),
    type_desc = as.factor(type_desc),
    day_of_week = factor(day_of_week,
                         levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
                         labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
                         ordered = TRUE),
    desc_ca = as.factor(desc_ca),
    light_condition = factor(light_condition,
                             levels = c(1, 2, 3, 4, 5, 6, 9),
                             labels = c("Day", 
                                        "Dusk/Dawn", 
                                        "Dark Street lights on", 
                                        "Dark Street lights off", 
                                        "Dark No street lights", 
                                        "Dark Street lights unknown", 
                                        "Unknown")),
    severity = factor(severity,
                      levels = c(1, 2, 3, 4),
                      labels = c("Fatal accident",
                                 "Serious injury accident",
                                 "Other injury accident",
                                 "Non injury accident")),
    police_attend = factor(police_attend,
                           levels = c(1, 2, 3),
                           labels = c("Yes", 
                                      "No", 
                                      "Unknown")),
    road_geometry_desc = as.factor(road_geometry_desc),
    weather_condition = as.factor(weather_condition),
    surface_condition = as.factor(surface_condition)
  )
```
#### 4.4 Check for duplicates

```{r,message=FALSE}
# Check for duplicate accident_id
duplicate_ids <- merged_data %>%
  group_by(accident_id) %>%
  filter(n() > 1)

nrow(duplicate_ids)
head(duplicate_ids)

# Remove all duplicate rows
merged_data %<>%
  distinct(accident_id, .keep_all = TRUE)  # Keep all columns, but only unique accident_id values
```



```{r,message=FALSE}
# compact view of our tidy data
glimpse(merged_data)

```


We removed `DAY_OF_WEEK`, `ACCIDENT_TYPE`, `RMA`, `DCA_CODE`, `NODE_ID` and `ROAD_GEOMETRY` from the accidents dataset as this information was either redundant or not relevant to our analysis. We renamed variables for clarity and consistency across datasets. For example, `ACCIDENT_NO` was renamed to `accident_id` to serve as a clear identifier across all datasets.
We removed a total of 6374 duplicates from the database.
    
##	5. Creating new variables 

We created a new variable `total_casualties` by summing `fatalities`, `serious_injuries`, and `minor_injuries`. We also extracted month and year from `date`.

```{r}

# Create total_casualties and extract month and year
merged_data %<>%
  mutate(
    total_casualties = fatalities + serious_injuries + minor_injuries,
    month = month(date, label = TRUE, abbr = TRUE),  # Extract month
    year = year(date)  # Extract year
  )

# Display the first few rows of only the new variables with accident_id.
head(merged_data %>% select(accident_id, total_casualties, month, year))

```
The month variable helps analyze trends and patterns across different months, while the year variable provides temporal context for the data.
The new total_casualties variable provides a single measure of accident severity in terms of human impact.


##	6. Handling missing values

In this step we will be handling missing values, checking for inconsistencies and standardizing missing and unclear values:

#### 6.1 Check for missing values

```{r}
# Check for missing values:
sum(is.na(merged_data))
colSums(is.na(merged_data))
```
#### 6.2 Check for inconsistencies or unexpected values

```{r, results='hide'}

# List of categorical variables to check for inconsistencies
categorical_vars <- c("type_desc", "day_of_week", "desc_ca",
                       "light_condition", "road_geometry_desc","police_attend",
                       "severity", "weather_condition", "surface_condition")

# Function to display unique values for categorical variables
check_categorical_vars <- function(data, variables) {
  for (var in variables) {
    cat("Unique values for", var, ":\n")
    print(table(data[[var]]))
    cat("\n")
  }
}

check_categorical_vars(merged_data, categorical_vars)
```
We identified 599 missing entries in the police_attend variable.


```{r}
# Check for missing values in 'police_attend'
missing_count <- sum(is.na(merged_data$police_attend))
cat("Number of missing values in 'police_attend':", missing_count, "\n")

# Replace NA values with 'Unknown'
merged_data$police_attend[is.na(merged_data$police_attend)] <- "Unknown"

# Display summary of 'police_attend'
summary(merged_data$police_attend)

# Replace "Not known" with "Unknown" in weather_condition
merged_data %<>%
  mutate(weather_condition = recode(weather_condition, "Not known" = "Unknown"))

# Replace "Unk." with "Unknown" in surface_condition
merged_data %<>%
  mutate(surface_condition = recode(surface_condition, "Unk." = "Unknown"))

```

* We identified and counted missing values in the dataset.
* Checked categorical variables for any inconsistencies or unexpected values.
* Replaced missing values in the severity column with "Unknown".
* Standardized categorical values by replacing specific codes ("Not known", "Unk.") with "Unknown" in the weather_condition and surface_condition columns.



##	7.Detect and Handle Outliers

In this section, we'll perform outlier detection for numeric columns to identify and address any extreme values that could affect the analysis. We will also visualize the data before and after removing outliers.

For each numeric variable, we'll calculate the Interquartile Range (IQR)[02] and define outliers as values outside 1.5 times the IQR from the quartiles.

#### 7.1 We will creat a list of all variables and a function to generate boxplots:

```{r, warning=FALSE}
# List of numeric variables:
numeric_vars <- c("no_of_vehicles", "fatalities", "serious_injuries",
                  "minor_injuries", "uninjured_people", "total_people_involved", 
                  "speed_zone")

# Create a function to generate boxplots
create_boxplots <- function(data, variables) {
  for (var in variables) {
    p <- ggplot(data, aes_string(y = var)) +
      geom_boxplot() +
      labs(title = paste("Boxplot of", var), y = var)
    print(p)
  }
}

create_boxplots(merged_data, numeric_vars)


```

    
#### 7.1.1 Outlier Detection **'no_of_vehicles'**:   

```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$no_of_vehicles, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$no_of_vehicles, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(no_of_vehicles < lower_fence | no_of_vehicles > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$no_of_vehicles)
outliers %<>%
  arrange(desc(no_of_vehicles))
head(outliers$no_of_vehicles)

```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$no_of_vehicles`.   
    
We repeat similar analysis for other numeric variables:   

#### 7.1.2 Outlier Detection **fatalities**:

```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$fatalities, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$fatalities, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(fatalities < lower_fence | fatalities > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$fatalities)
outliers %<>%
  arrange(desc(fatalities))
head(outliers$fatalities)
```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$fatalities`.

#### 7.1.3 Outlier Detection **serious_injuries**
```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$serious_injuries, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$serious_injuries, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(serious_injuries < lower_fence | serious_injuries > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$serious_injuries)
outliers %<>%
  arrange(desc(serious_injuries))
head(outliers$serious_injuries)
```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$serious_injuries`.


#### 7.1.4 Outlier Detection **minor_injuries**
```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$minor_injuries, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$minor_injuries, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(minor_injuries < lower_fence | minor_injuries > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$minor_injuries)
outliers %<>%
  arrange(desc(minor_injuries))
head(outliers$minor_injuries)
```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$minor_injuries`.

#### 7.1.5 Outlier Detection **uninjured_people**
```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$uninjured_people, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$uninjured_people, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(uninjured_people < lower_fence | uninjured_people > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$uninjured_people)
outliers %<>%
  arrange(desc(uninjured_people))
head(outliers$uninjured_people)

```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$uninjured_people`.


#### 7.1.6 Outlier Detection **total_people_involved**

```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$total_people_involved, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$total_people_involved, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(total_people_involved < lower_fence | total_people_involved > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Analyse outliers:
summary(outliers$total_people_involved)
outliers %<>%
  arrange(desc(total_people_involved))
head(outliers$total_people_involved)
```
The values appear realistic, and no additional action is needed for the outliers in `merged_data$total_people_involved`.


#### 7.1.7 Outlier Detection **speed_zone**:    

In this section, we will compare the number of rows in the dataset before and after the removal of outliers in the speed_zone variable. We will also summarize the changes.
```{r}
# Calculate Q1 (25th percentile) and Q3 (75th percentile):
Q1 <- quantile(merged_data$speed_zone, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_data$speed_zone, 0.75, na.rm = TRUE)

# Calculate IQR (Interquartile Range)
IQR <- Q3 - Q1

# Define outlier fence:
lower_fence <- Q1 - 1.5 * IQR
upper_fence <- Q3 + 1.5 * IQR

# Print number of outliers:
cat("Lower Fence for Outliers:", lower_fence, "\n")
cat("Upper Fence for Outliers:", upper_fence, "\n")

# Find outliers:
outliers <- merged_data %>%
  filter(speed_zone < lower_fence | speed_zone > upper_fence)

# Count the number of outliers:
num_outliers <- nrow(outliers)
cat("Number of Outliers:", num_outliers, "\n")

# Examine outliers
print(table(outliers$speed_zone))

# Remove outliers
merged_data_clean <- merged_data %>%
  filter(speed_zone >= lower_fence & speed_zone <= upper_fence)
# 7. Visualize the speed_zone distribution after removing outliers
ggplot(merged_data_clean, aes(x = speed_zone)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.7) +
  labs(title = "Distribution of Speed Zones After Outlier Removal",
       x = "Speed Zone", y = "Count")

# 8. Compare before and after
cat("Rows before outlier removal:", nrow(merged_data), "\n")
cat("Rows after outlier removal:", nrow(merged_data_clean), "\n")
cat("Rows removed:", nrow(merged_data) - nrow(merged_data_clean), "\n")

# 9. Summary of clean data
summary(merged_data_clean$speed_zone)

merged_data <- merged_data_clean


```
For the `speed_zone` variable, we identified outliers above 110 km/h. These values are inconsistent with Victoria's standard speed limits. We removed a total of 12140 outliers as they likely represent data entry errors.

For all other outliers, we chose to retain them. Despite their appearance as outliers on the boxplot, these values remain realistic. This is because, in most accidents, the number of vehicles or persons involved is typically low, which aligns with the observed data distribution.



##	8. Transform the Data

In this section, we will explore the distribution of numeric variables and apply transformations to normalize the data if necessary.    

#### 8.1 Histogram Visualization

We will plot histograms for the total_casualties and no_of_vehicles columns before and after applying a log transformation.


```{r}

# Create histograms for 'total_casualties' before and after log transformation

# Original distribution of 'total_casualties'
p1<- hist(merged_data$total_casualties,
     breaks = 30,
     main = "Distribution of Total Casualties",
     xlab = "Total Casualties",
     col = "blue",
     border = "black")

# Distribution of log-transformed 'total_casualties'
p2<- hist(log(merged_data$total_casualties),
     breaks = 30,
     main = "Distribution of Log(Total Casualties)",
     xlab = "Log(Total Casualties)",
     col = "green",
     border = "black")



# Create histograms for 'no_of_vehicles' before and after log transformation

# Original distribution of 'no_of_vehicles'
p1<- hist(merged_data$no_of_vehicles,
     breaks = 30,
     main = "Distribution of Number of Vehicles",
     xlab = "Number of Vehicles",
     col = "blue",
     border = "black")

# Distribution of log-transformed 'no_of_vehicles'
p2<- hist(log(merged_data$no_of_vehicles),
     breaks = 30,
     main = "Distribution of Log(Number of Vehicles)",
     xlab = "Log(Number of Vehicles)",
     col = "green",
     border = "black")


```
The log transformation of `no_of_vehicles` and `total_casualties` helps normalize the data distribution. This transformation improves the interpretation of relationships between variables by stabilizing variance and making the distribution more symmetrical.



## 9. Presentation link

Include the link to your video walkthrough here. 

## 10. References

[01]Department of Transport (2024) *Victoria Road Crash Data*, Discover Victoria's Open Data website, accessed 8 August 2024. https://discover.data.vic.gov.au/dataset/victoria-road-crash-data

[02]Iglewicz B, Hoaglin DC (1993) How to detect and handle outliers, Wiley, New York.


<br>
<br>












